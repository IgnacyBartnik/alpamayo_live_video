{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a39e5a5",
   "metadata": {},
   "source": [
    "# Alpamayo Live Inference + MJPEG Stream\n",
    "\n",
    "This notebook reuses the same dataset loading, preprocessing and inference calls from `notebooks/inference.ipynb` and adds a simple real-time simulation loop plus a Flask MJPEG server on localhost:8000 that always serves the most recent rendered frame.\n",
    "\n",
    "Notes:\n",
    "- `model loading` happens in cell 3\n",
    "- `dataset loading` happens in cell 5\n",
    "- `inference` happens in cell 7\n",
    "- `rendering` happens in cell 7\n",
    "- `streaming` (Flask MJPEG background server) starts in cell 9\n",
    "\n",
    "Run cells sequentially. The loop in cell 8 processes frames repeatedly to simulate a live feed; you can stop it by interrupting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a4101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports and small utilities\n",
    "import time\n",
    "import threading\n",
    "import io\n",
    "import itertools\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "from flask import Flask, Response, make_response\n",
    "\n",
    "# Repo utilities (reuse existing preprocessing/inference utilities)\n",
    "from alpamayo_r1.models.alpamayo_r1 import AlpamayoR1\n",
    "from alpamayo_r1.load_physical_aiavdataset import load_physical_aiavdataset\n",
    "from alpamayo_r1 import helper\n",
    "\n",
    "# Shared variable to hold the latest JPEG frame bytes served by MJPEG stream\n",
    "latest_frame = { 'jpg': None, 'lock': threading.Lock() }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db9f587",
   "metadata": {},
   "source": [
    "## Model loading (observe model load time)\n",
    "\n",
    "This cell loads the pretrained `AlpamayoR1` model and the `processor` used for preprocessing. Loading may be slow depending on model size and whether GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51974a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load model (this may take time)\n",
    "start = time.time()\n",
    "# NOTE: uses the same call as the reference notebook\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "try:\n",
    "    model = AlpamayoR1.from_pretrained('nvidia/Alpamayo-R1-10B', dtype=torch.bfloat16 if device == 'cuda' else torch.float32).to(device)\n",
    "    print('Loaded model on', device)\n",
    "except Exception as e:\n",
    "    # If large model cannot be loaded in test environment, raise descriptive error\n",
    "    raise RuntimeError('Model load failed; ensure weights are available and you have the right runtime. Error: ' + str(e))\n",
    "processor = helper.get_processor(model.tokenizer)\n",
    "print(f'Model load time: {time.time() - start:.1f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99b5be",
   "metadata": {},
   "source": [
    "## Dataset loading and buffer initialization\n",
    "\n",
    "This cell loads the exact same clip and data used in the reference notebook. We keep a rolling temporal buffer of the camera frames (same `num_frames` as the loader, typically 4). The buffer will be advanced each step to simulate realtime playback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bccea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Load dataset (same footage as original notebook)\n",
    "import pandas as pd\n",
    "# Attempt to find clip ids; adjust path if needed\n",
    "try:\n",
    "    clip_ids = pd.read_parquet('clip_ids.parquet')[\"clip_id\"].tolist()\n",
    "    clip_id = clip_ids[774]\n",
    "except Exception as e:\n",
    "    # Fallback: use a default clip id or raise helpful message\n",
    "    raise RuntimeError('Could not read clip_ids.parquet. Ensure the dataset is present. Error: ' + str(e))\n",
    "print('Using clip_id:', clip_id)\n",
    "# load data using the repo helper\n",
    "data = load_physical_aiavdataset(clip_id)\n",
    "# data keys: image_frames (N_cameras, num_frames, 3, H, W), ego_history_xyz, ego_future_xyz, etc.\n",
    "print({k: (v.shape if hasattr(v, 'shape') else type(v)) for k, v in data.items() if k in ['image_frames','ego_history_xyz','ego_future_xyz']})\n",
    "\n",
    "# Determine camera we want to display as the main camera (front wide is camera index 1 by loader mapping)\n",
    "cam_indices = data['camera_indices']\n",
    "try:\n",
    "    main_cam_pos = (cam_indices == 1).nonzero(as_tuple=True)[0].item()\n",
    "except Exception:\n",
    "    # Fallback to first camera available\n",
    "    main_cam_pos = 0\n",
    "print('Main camera position in returned tensor:', main_cam_pos)\n",
    "\n",
    "# Image buffer: copy the tensor to CPU numpy for easy drawing/cycling and convert to HWC uint8 per frame\n",
    "image_frames = data['image_frames']  # (N_cameras, num_frames, 3, H, W)\n",
    "N_cams, num_frames, C, H, W = image_frames.shape\n",
    "print(f'Loaded images: N_cams={N_cams}, num_frames={num_frames}, H={H}, W={W}')\n",
    "# Convert to numpy HWC uint8 for each camera/time: shape (N_cams, num_frames, H, W, 3)\n",
    "image_frames_np = image_frames.cpu().numpy().transpose(0,1,3,4,2).astype('uint8')\n",
    "# Create a simple circular iterator over time indices to simulate playback (we'll advance buffer each step)\n",
    "time_idx_iter = itertools.cycle(range(num_frames))\n",
    "\n",
    "# Rolling buffer state: for each camera keep the loaded frames (we'll rotate them to simulate streaming)\n",
    "buffer = image_frames_np.copy()  # shape (N_cams, num_frames, H, W, 3)\n",
    "\n",
    "# Keep a device-resident copy of the ego history used for inference\n",
    "ego_history_xyz = data['ego_history_xyz']  # (1,1,T,3)\n",
    "ego_history_rot = data['ego_history_rot']  # (1,1,T,3,3)\n",
    "\n",
    "print('Buffer initialized. Ready to run inference loop.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7646b7ce",
   "metadata": {},
   "source": [
    "## Inference + rendering loop\n",
    "\n",
    "This cell runs the realtime simulation loop. Each iteration it:\n",
    "- advances the rolling buffer of image frames (cycles through available frames),\n",
    "- builds `messages` using `helper.create_message` (same preprocessing as reference),\n",
    "- tokenizes with `processor.apply_chat_template`,\n",
    "- calls `model.sample_trajectories_from_data_with_vlm_rollout` to get predicted trajectories and reasoning traces,\n",
    "- renders a composed image (left: current camera image, right: BEV visualization of predicted trajectory),\n",
    "- updates `latest_frame` bytes for the MJPEG server to serve.\n",
    "\n",
    "You can run this cell; it will loop until you interrupt the kernel. For demo purposes the loop cycles the small set of frames returned by the loader to simulate continuous playback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Inference + rendering loop (run to start simulated realtime processing)\n",
    "from copy import deepcopy\n",
    "import base64\n",
    "from IPython.display import display, Image as IPImage, clear_output\n",
    "\n",
    "# Toggle: show live video inside the notebook output (clears and updates the cell output)\n",
    "SHOW_IN_NOTEBOOK = True\n",
    "\n",
    "def frame_to_jpeg_bytes(img_np, quality=90):\n",
    "    # img_np: H,W,3 uint8 (RGB) -> JPEG bytes\n",
    "    is_success, buffer = cv2.imencode('.jpg', cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR), [int(cv2.IMWRITE_JPEG_QUALITY), quality])\n",
    "    if not is_success:\n",
    "        raise RuntimeError('JPEG encoding failed')\n",
    "    return buffer.tobytes()\n",
    "\n",
    "def render_bev(pred_xy, gt_xy=None, size=(H, W)):\n",
    "    # Create a simple BEV panel as PIL image and draw the predicted trajectory (meters -> pixels scaled)\n",
    "    bev_w, bev_h = size[1], size[0]\n",
    "    img = Image.new('RGB', (bev_w, bev_h), (255,255,255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    if pred_xy is None:\n",
    "        return img\n",
    "    pts = np.array(pred_xy)  # T,2\n",
    "    # compute mins/maxs and elementwise span (avoid using Python max on arrays)\n",
    "    mins = pts.min(axis=0)\n",
    "    maxs = pts.max(axis=0)\n",
    "    span = np.maximum(maxs - mins, 1e-3)\n",
    "    mins = mins - 0.1 * span\n",
    "    maxs = maxs + 0.1 * span\n",
    "    span = maxs - mins\n",
    "    def to_px(xy):\n",
    "        rel = (xy - mins) / span\n",
    "        px = np.stack([rel[:,0] * (bev_w-20) + 10, (1.0 - rel[:,1]) * (bev_h-20) + 10], axis=1)\n",
    "        return px.astype(int)\n",
    "    px = to_px(pts)\n",
    "    for i in range(len(px)-1):\n",
    "        draw.line([tuple(px[i]), tuple(px[i+1])], fill=(0,0,255), width=3)\n",
    "    for p in px:\n",
    "        draw.ellipse((p[0]-3,p[1]-3,p[0]+3,p[1]+3), fill=(0,0,255))\n",
    "    if gt_xy is not None:\n",
    "        gpx = to_px(np.array(gt_xy))\n",
    "        for i in range(len(gpx)-1):\n",
    "            draw.line([tuple(gpx[i]), tuple(gpx[i+1])], fill=(255,0,0), width=2)\n",
    "    return img\n",
    "\n",
    "# def compose_frame(current_rgb, bev_img, text_lines=None, debug_stamp=None):\n",
    "#     left = Image.fromarray(current_rgb)\n",
    "#     bev_resized = bev_img.resize((left.width, left.height))\n",
    "#     out = Image.new('RGB', (left.width + bev_resized.width, left.height))\n",
    "#     out.paste(left, (0,0))\n",
    "#     out.paste(bev_resized, (left.width,0))\n",
    "#     draw = ImageDraw.Draw(out)\n",
    "#     if text_lines:\n",
    "#         font = ImageFont.load_default()\n",
    "#         x = 6\n",
    "#         y = 6\n",
    "#         for line in text_lines[:6]:\n",
    "#             draw.text((x,y), line, fill=(0,0,0), font=font)\n",
    "#             y += 12\n",
    "#     if debug_stamp:\n",
    "#         draw.text((out.width-150, out.height-20), debug_stamp, fill=(0,0,0))\n",
    "#     return np.array(out)\n",
    "\n",
    "\n",
    "def compose_frame(current_rgb, bev_img, text_lines=None, debug_stamp=None):\n",
    "    # Convert main frame to PIL\n",
    "    left = Image.fromarray(current_rgb)\n",
    "\n",
    "    # Rotate BEV image 90Â° counterclockwise\n",
    "    bev_rotated = bev_img.rotate(90, expand=True)\n",
    "\n",
    "    # Scale BEV to 1/5 of main image size\n",
    "    overlay_w = left.width // 5\n",
    "    overlay_h = left.height // 2\n",
    "    bev_small = bev_rotated.resize((overlay_w, overlay_h))\n",
    "\n",
    "    # Create overlay background (white) slightly bigger than BEV\n",
    "    padding = 10\n",
    "    bg_w = overlay_w + padding * 2\n",
    "    bg_h = overlay_h + padding * 2 + 20  # extra space for title\n",
    "    overlay_bg = Image.new('RGB', (bg_w, bg_h), (255, 255, 255))\n",
    "\n",
    "    # Paste BEV onto background\n",
    "    overlay_bg.paste(bev_small, (padding, padding + 20))\n",
    "\n",
    "    \n",
    "    # Add title\n",
    "    draw_overlay = ImageDraw.Draw(overlay_bg)\n",
    "    font = ImageFont.load_default(size = 28)\n",
    "    title = \"Projected path\"\n",
    "    \n",
    "    # Use textbbox to measure text size\n",
    "    bbox = draw_overlay.textbbox((0, 0), title, font=font)\n",
    "    text_w = bbox[2] - bbox[0]\n",
    "    text_h = bbox[3] - bbox[1]\n",
    "    \n",
    "    draw_overlay.text(((bg_w - text_w) // 2, 5), title, fill=(0, 0, 0), font=font)\n",
    "\n",
    "    # Compose final image: start with main frame\n",
    "    out = left.copy()\n",
    "\n",
    "    # Paste overlay in top-left corner\n",
    "    out.paste(overlay_bg, (10, 10))\n",
    "\n",
    "    # Draw text lines on main image if provided\n",
    "    draw_main = ImageDraw.Draw(out)\n",
    "    if text_lines:\n",
    "        x = 6\n",
    "        y = out.height - 12 * min(len(text_lines), 6) - 6\n",
    "        for line in text_lines[:6]:\n",
    "            draw_main.text((x, y), line, fill=(0, 0, 0), font=font)\n",
    "            y += 12\n",
    "\n",
    "    # Debug stamp\n",
    "    if debug_stamp:\n",
    "        draw_main.text((out.width - 150, out.height - 20), debug_stamp, fill=(0, 0, 0))\n",
    "\n",
    "    return np.array(out)\n",
    "\n",
    "\n",
    "# Main loop: each iteration advances buffer (cycle) and runs inference\n",
    "running = True\n",
    "step = 0\n",
    "\n",
    "print(f\"running is {running}\")\n",
    "\n",
    "# If showing in-notebook, display a placeholder (this will be updated via clear_output)\n",
    "if SHOW_IN_NOTEBOOK:\n",
    "    display(IPImage(data=frame_to_jpeg_bytes(np.zeros((H, W*2, 3), dtype='uint8'))))\n",
    "\n",
    "try:\n",
    "    while running:\n",
    "        start = time.time()\n",
    "        print(f\"step {step}\")\n",
    "        # Advance buffer: pop leftmost time index and append next frame from loaded frames to simulate new arrival\n",
    "        next_t = next(time_idx_iter)\n",
    "        buffer = np.roll(buffer, -1, axis=1)  # shift time dimension left\n",
    "        buffer[:, -1, ...] = image_frames_np[:, next_t, ...]  # append the frame at next_t\n",
    "\n",
    "        # Create messages from flattened frames (same call as reference notebook)\n",
    "        frames_for_message = torch.from_numpy(buffer).permute(0,1,4,2,3).contiguous()  # N_cams, num_frames, C, H, W\n",
    "        messages = helper.create_message(frames_for_message.flatten(0,1))\n",
    "\n",
    "        inputs = processor.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=False,\n",
    "            continue_final_message=True,\n",
    "            return_dict=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        model_inputs = {\n",
    "            'tokenized_data': inputs,\n",
    "            'ego_history_xyz': ego_history_xyz.to(device),\n",
    "            'ego_history_rot': ego_history_rot.to(device),\n",
    "        }\n",
    "        model_inputs = helper.to_device(model_inputs, device)\n",
    "\n",
    "        # Run model inference (may be slow); this uses the same sampling call as the reference notebook\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.manual_seed_all(42)\n",
    "        with torch.autocast(device if device=='cuda' else 'cpu', dtype=torch.bfloat16 if device=='cuda' else torch.float32):\n",
    "            with torch.no_grad():\n",
    "                pred_xyz, pred_rot, extra = model.sample_trajectories_from_data_with_vlm_rollout(\n",
    "                    data=deepcopy(model_inputs),\n",
    "                    top_p=0.98,\n",
    "                    temperature=0.6,\n",
    "                    num_traj_samples=1,\n",
    "                    max_generation_length=256,\n",
    "                    return_extra=True,\n",
    "                )\n",
    "\n",
    "        pred_xy = pred_xyz.cpu().numpy()[0,0,0,:, :2]  # (T,2)\n",
    "\n",
    "        reasoning_text = None\n",
    "        if isinstance(extra, dict) and 'cot' in extra and extra['cot'] is not None:\n",
    "            try:\n",
    "                reasoning_text = extra['cot'][0]\n",
    "            except Exception:\n",
    "                reasoning_text = None\n",
    "\n",
    "        current_rgb = buffer[main_cam_pos, -1]  # H,W,3 uint8 (RGB)\n",
    "\n",
    "        bev_img = render_bev(pred_xy, gt_xy=None, size=(H, W))\n",
    "        text_lines = None\n",
    "        if reasoning_text is not None:\n",
    "            if isinstance(reasoning_text, (list, tuple)):\n",
    "                text_lines = [str(x) for x in reasoning_text]\n",
    "            else:\n",
    "                text_lines = str(reasoning_text).split('\\n')\n",
    "\n",
    "        composed = compose_frame(current_rgb, bev_img, text_lines=text_lines, debug_stamp=f'step:{step}')\n",
    "\n",
    "        jpg = frame_to_jpeg_bytes(composed)\n",
    "        with latest_frame['lock']:\n",
    "            latest_frame['jpg'] = jpg\n",
    "\n",
    "        # Update the in-notebook display (clears previous output and shows the latest frame)\n",
    "        if SHOW_IN_NOTEBOOK:\n",
    "            try:\n",
    "                clear_output(wait=True)\n",
    "                display(IPImage(data=jpg))\n",
    "            except Exception:\n",
    "                # If display fails for any reason, continue without crashing the loop\n",
    "                pass\n",
    "                \n",
    "        print(f'Step procesing time: {time.time() - start:.1f}s')\n",
    "        step += 1\n",
    "        # time.sleep(0.2)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by user, stopping loop')\n",
    "except Exception as e:\n",
    "    print('Loop error:', e)\n",
    "finally:\n",
    "    running = False\n",
    "    print('Inference loop finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6061143b",
   "metadata": {},
   "source": [
    "## Start Flask MJPEG server in background thread\n",
    "\n",
    "This cell starts a minimal Flask app that serves `/video` as an MJPEG stream. The app reads from `latest_frame['jpg']` updated by the inference loop. The Flask server runs in a background thread so the notebook remains responsive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe552b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Start Flask server (run this cell after you've started the inference loop)\n",
    "app = Flask('alpamayo_stream')\n",
    "\n",
    "def mjpeg_generator():\n",
    "    boundary = '--frame'\n",
    "    while True:\n",
    "        with latest_frame['lock']:\n",
    "            jpg = latest_frame.get('jpg')\n",
    "        if jpg is None:\n",
    "            time.sleep(0.05)\n",
    "            continue\n",
    "        yield (b'%s\n",
    "Content-Type: image/jpeg\n",
    "Content-Length: %d\n",
    "\n",
    "' % (boundary.encode(), len(jpg))) + jpg + b'\n",
    "'\n",
    "\n",
    "@app.route('/video')\n",
    "def video_route():\n",
    "    return Response(mjpeg_generator(), mimetype='multipart/x-mixed-replace; boundary=--frame')\n",
    "\n",
    "def run_flask():\n",
    "    app.run(host='0.0.0.0', port=8000, threaded=True)\n",
    "\n",
    "server_thread = threading.Thread(target=run_flask, daemon=True)\n",
    "server_thread.start()\n",
    "print('Flask MJPEG server started at http://localhost:8000/video')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36248b1a",
   "metadata": {},
   "source": [
    "## Notes and how to view\n",
    "- Start the model load cell (4), dataset load (6), then run the inference loop (8) to begin producing frames. Then run the Flask server cell (10).\n",
    "- The MJPEG endpoint is: http://localhost:8000/video\n",
    "- To SSH-tunnel port 8000 from a remote machine, forward local port 8000 (example):\n",
    "\n",
    "```\n",
    "# on your workstation (example SSH command):\n",
    "ssh -L 8000:localhost:8000 user@remote-host\n",
    "```\n",
    "\n",
    "Troubleshooting: If model weights are large and you cannot load them, you can still run the rendering loop by mocking `pred_xy` (e.g., simple circle) and setting `latest_frame['jpg']` yourself. The notebook intentionally keeps calls in clear cells so you can replace or step through them."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
